{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPpvQK4aSrv6RR7XneoQMPf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnupKhanal01/webscrape/blob/main/Data_Scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTyjU6ktOhfl",
        "outputId": "0121804b-df79-4958-ad0d-91929d7fd618"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Scheduler started. Scraping every 24 hours.\n",
            "Starting scraping...\n",
            "Data saved successfully to /content/drive/My Drive/Colab Notebooks/Scraped Data/juniper_vs_moshy_data.csv\n",
            "Waiting 24 hours before the next scrape...\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive to save the output CSV file\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the directory where the CSV file will be saved\n",
        "SAVE_DIRECTORY = \"/content/drive/My Drive/Colab Notebooks/Scraped Data\"\n",
        "OUTPUT_FILE = os.path.join(SAVE_DIRECTORY, \"juniper_vs_moshy_data.csv\")\n",
        "\n",
        "# Function to scrape Trustpilot reviews\n",
        "def scrape_trustpilot_reviews():\n",
        "    url = \"https://au.trustpilot.com/review/myjuniper.com\"\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"}\n",
        "    reviews = []\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Locate review sections\n",
        "        review_sections = soup.find_all('div', class_='styles_reviewContent__0Q2Tg')\n",
        "        for section in review_sections:\n",
        "            review_text = section.find('p').text.strip()\n",
        "            reviews.append(review_text)  # Collect all reviews\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping Trustpilot: {e}\")\n",
        "    return reviews\n",
        "\n",
        "# Function to scrape details from Get Moshy\n",
        "def scrape_getmoshy_details():\n",
        "    url = \"https://www.getmoshy.com.au/\"\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"}\n",
        "    data = {\"Money Back Guarantee\": \"\", \"Price\": \"\"}\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Extract relevant information\n",
        "        guarantee_section = soup.find(string=lambda t: t and \"money back\" in t.lower())\n",
        "        if guarantee_section:\n",
        "            data[\"Money Back Guarantee\"] = guarantee_section.strip()\n",
        "\n",
        "        price_section = soup.find(string=lambda t: t and \"price\" in t.lower())\n",
        "        if price_section:\n",
        "            data[\"Price\"] = price_section.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping Get Moshy: {e}\")\n",
        "    return data\n",
        "\n",
        "# Function to scrape details from Youly\n",
        "def scrape_youly_details():\n",
        "    url = \"https://youly.com.au/treatment/weight-loss/\"\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"}\n",
        "    reviews = []\n",
        "    data = {\"Money Back Guarantee\": \"\", \"Price\": \"\"}\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Extract all reviews\n",
        "        review_sections = soup.find_all('div', class_='review-text')\n",
        "        for section in review_sections:\n",
        "            review_text = section.text.strip()\n",
        "            reviews.append(review_text)\n",
        "\n",
        "        # Extract guarantee and price\n",
        "        guarantee_section = soup.find(string=lambda t: t and \"money back\" in t.lower())\n",
        "        if guarantee_section:\n",
        "            data[\"Money Back Guarantee\"] = guarantee_section.strip()\n",
        "\n",
        "        price_section = soup.find(string=lambda t: t and \"price\" in t.lower())\n",
        "        if price_section:\n",
        "            data[\"Price\"] = price_section.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping Youly: {e}\")\n",
        "    return reviews, data\n",
        "\n",
        "# Main function to scrape all websites and save to CSV\n",
        "def scrape_all_sites():\n",
        "    print(\"Starting scraping...\")\n",
        "\n",
        "    # Scrape data from Trustpilot\n",
        "    trustpilot_reviews = scrape_trustpilot_reviews()\n",
        "\n",
        "    # Scrape data from Get Moshy\n",
        "    getmoshy_data = scrape_getmoshy_details()\n",
        "\n",
        "    # Scrape data from Youly\n",
        "    youly_reviews, youly_data = scrape_youly_details()\n",
        "\n",
        "    # Combine data into a DataFrame\n",
        "    data = {\n",
        "        \"Source\": [\"Trustpilot\"] * len(trustpilot_reviews) + [\"Youly\"] * len(youly_reviews),\n",
        "        \"Reviews\": trustpilot_reviews + youly_reviews,\n",
        "        \"Money Back Guarantee\": [\"\"] * len(trustpilot_reviews) + [youly_data[\"Money Back Guarantee\"]] * len(youly_reviews),\n",
        "        \"Price\": [\"\"] * len(trustpilot_reviews) + [youly_data[\"Price\"]] * len(youly_reviews)\n",
        "    }\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Add Get Moshy details to the DataFrame\n",
        "    moshy_row = pd.DataFrame({\n",
        "        \"Source\": [\"Get Moshy\"],\n",
        "        \"Reviews\": [\"\"],\n",
        "        \"Money Back Guarantee\": [getmoshy_data[\"Money Back Guarantee\"]],\n",
        "        \"Price\": [getmoshy_data[\"Price\"]]\n",
        "    })\n",
        "    df = pd.concat([df, moshy_row], ignore_index=True)\n",
        "\n",
        "    # Ensure the directory exists\n",
        "    if not os.path.exists(SAVE_DIRECTORY):\n",
        "        os.makedirs(SAVE_DIRECTORY)\n",
        "\n",
        "    # Save DataFrame to CSV\n",
        "    df.to_csv(OUTPUT_FILE, index=False)\n",
        "    print(f\"Data saved successfully to {OUTPUT_FILE}\")\n",
        "\n",
        "# Scheduler function\n",
        "def schedule_scraping(interval_hours=24):\n",
        "    print(f\"Scheduler started. Scraping every {interval_hours} hours.\")\n",
        "    while True:\n",
        "        scrape_all_sites()\n",
        "        print(f\"Waiting {interval_hours} hours before the next scrape...\")\n",
        "        time.sleep(interval_hours * 3600)\n",
        "\n",
        "# Run the scheduler\n",
        "if __name__ == \"__main__\":\n",
        "    schedule_scraping(interval_hours=24)\n"
      ]
    }
  ]
}